{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74db8f60ee04d21a41c64e304748afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mosaicml import *\n",
    "from mosaicml.constants import MLModelFlavours\n",
    "from werkzeug.datastructures import FileStorage\n",
    "import pandas as pd\n",
    "import os, tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d86d51a9aa4691a9f00e95f8f90354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# print(\"-----creating spark session\")\n",
    "# spark = SparkSession.builder.appName(\"jupyter-nb-test\").getOrCreate()\n",
    "# spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print(\"-----reading data from azure\")\n",
    "storage_account_name = \"pocmosaic\"\n",
    "storage_account_access_key = \"iX6jZejeNh1bUejdSRYk+4X2S/pAbTy+idkTM5UAw2HxgxSnaYGJSN+ffzZAQ5/Q6xQe2Ja74Re9nKHMVmsGhA==\"\n",
    "file_location = \"wasbs://container1@pocmosaic.blob.core.windows.net/catalog/local_file_upload/RatanBBostonHousing.csv\"\n",
    "file_type = \"csv\"\n",
    "spark.conf.set(\"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",storage_account_access_key)\n",
    "dataset = spark.read.option(\"header\", \"true\").format(file_type).option(\"inferSchema\", \"true\").load(file_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 1 unexpectedly reached final status 'error'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "21/06/26 14:48:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://livy-spark-6c796bc968-n8v52:4041\n",
      "21/06/26 14:48:59 INFO SparkContext: Added JAR file:///app/apache-livy-0.6.0-incubating-bin/rsc-jars/livy-api-0.6.0-incubating.jar at spark://livy-spark-6c796bc968-n8v52:40082/jars/livy-api-0.6.0-incubating.jar with timestamp 1624718939160\n",
      "21/06/26 14:48:59 INFO SparkContext: Added JAR file:///app/apache-livy-0.6.0-incubating-bin/rsc-jars/livy-rsc-0.6.0-incubating.jar at spark://livy-spark-6c796bc968-n8v52:40082/jars/livy-rsc-0.6.0-incubating.jar with timestamp 1624718939161\n",
      "21/06/26 14:48:59 INFO SparkContext: Added JAR file:///app/apache-livy-0.6.0-incubating-bin/rsc-jars/livy-thriftserver-session-0.6.0-incubating.jar at spark://livy-spark-6c796bc968-n8v52:40082/jars/livy-thriftserver-session-0.6.0-incubating.jar with timestamp 1624718939161\n",
      "21/06/26 14:48:59 INFO SparkContext: Added JAR file:///app/apache-livy-0.6.0-incubating-bin/rsc-jars/netty-all-4.0.37.Final.jar at spark://livy-spark-6c796bc968-n8v52:40082/jars/netty-all-4.0.37.Final.jar with timestamp 1624718939161\n",
      "21/06/26 14:48:59 INFO SparkContext: Added JAR file:///app/apache-livy-0.6.0-incubating-bin/repl_2.11-jars/commons-codec-1.9.jar at spark://livy-spark-6c796bc968-n8v52:40082/jars/commons-codec-1.9.jar with timestamp 1624718939161\n",
      "21/06/26 14:48:59 INFO SparkContext: Added JAR file:///app/apache-livy-0.6.0-incubating-bin/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar at spark://livy-spark-6c796bc968-n8v52:40082/jars/livy-core_2.11-0.6.0-incubating.jar with timestamp 1624718939161\n",
      "21/06/26 14:48:59 INFO SparkContext: Added JAR file:///app/apache-livy-0.6.0-incubating-bin/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar at spark://livy-spark-6c796bc968-n8v52:40082/jars/livy-repl_2.11-0.6.0-incubating.jar with timestamp 1624718939161\n",
      "21/06/26 14:48:59 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...\n",
      "21/06/26 14:48:59 INFO TransportClientFactory: Successfully created connection to spark-master/10.96.57.1:7077 after 9 ms (0 ms spent in bootstraps)\n",
      "21/06/26 14:48:59 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210626144933-0005\n",
      "21/06/26 14:48:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43041.\n",
      "21/06/26 14:48:59 INFO NettyBlockTransferService: Server created on livy-spark-6c796bc968-n8v52:43041\n",
      "21/06/26 14:48:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/06/26 14:48:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, livy-spark-6c796bc968-n8v52, 43041, None)\n",
      "21/06/26 14:48:59 INFO BlockManagerMasterEndpoint: Registering block manager livy-spark-6c796bc968-n8v52:43041 with 366.3 MB RAM, BlockManagerId(driver, livy-spark-6c796bc968-n8v52, 43041, None)\n",
      "21/06/26 14:48:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, livy-spark-6c796bc968-n8v52, 43041, None)\n",
      "21/06/26 14:48:59 INFO BlockManager: external shuffle service port = 7337\n",
      "21/06/26 14:48:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, livy-spark-6c796bc968-n8v52, 43041, None)\n",
      "21/06/26 14:48:59 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "21/06/26 14:48:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "21/06/26 14:48:59 INFO SparkEntries: Spark context finished initialization in 1422ms\n",
      "21/06/26 14:48:59 INFO SparkEntries: Created Spark session.\n",
      "21/06/26 14:49:06 INFO SparkEntries: Created SQLContext.\n",
      "21/06/26 14:49:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/app/apache-livy-0.6.0-incubating-bin/spark-warehouse').\n",
      "21/06/26 14:49:28 INFO SharedState: Warehouse path is 'file:/app/apache-livy-0.6.0-incubating-bin/spark-warehouse'.\n",
      "21/06/26 14:49:29 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "21/06/26 14:49:29 INFO Version: Elasticsearch Hadoop v5.1.1 [c94f085010]\n",
      "21/06/26 14:49:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-azure-file-system.properties,hadoop-metrics2.properties\n",
      "21/06/26 14:49:29 INFO MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n",
      "21/06/26 14:49:29 INFO MetricsSystemImpl: azure-file-system metrics system started\n",
      "21/06/26 14:49:32 INFO FileSourceStrategy: Pruning directories with: \n",
      "21/06/26 14:49:32 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "21/06/26 14:49:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "21/06/26 14:49:32 INFO FileSourceScanExec: Pushed Filters: \n",
      "21/06/26 14:49:32 INFO CodeGenerator: Code generated in 174.323334 ms\n",
      "21/06/26 14:49:33 INFO CodeGenerator: Code generated in 15.012038 ms\n",
      "21/06/26 14:49:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 286.0 KB, free 366.0 MB)\n",
      "21/06/26 14:49:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.3 KB, free 366.0 MB)\n",
      "21/06/26 14:49:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on livy-spark-6c796bc968-n8v52:43041 (size: 24.3 KB, free: 366.3 MB)\n",
      "21/06/26 14:49:33 INFO SparkContext: Created broadcast 0 from load at NativeMethodAccessorImpl.java:0\n",
      "21/06/26 14:49:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4230437 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/06/26 14:49:33 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0\n",
      "21/06/26 14:49:33 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/06/26 14:49:33 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)\n",
      "21/06/26 14:49:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/06/26 14:49:33 INFO DAGScheduler: Missing parents: List()\n",
      "21/06/26 14:49:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/06/26 14:49:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 366.0 MB)\n",
      "21/06/26 14:49:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.0 MB)\n",
      "21/06/26 14:49:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on livy-spark-6c796bc968-n8v52:43041 (size: 4.5 KB, free: 366.3 MB)\n",
      "21/06/26 14:49:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "21/06/26 14:49:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/06/26 14:49:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "21/06/26 14:49:34 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)\n",
      "21/06/26 14:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210626144933-0005/0 on worker-20210611185624-10.244.6.143-38083 (10.244.6.143:38083) with 2 core(s)\n",
      "21/06/26 14:49:34 INFO StandaloneSchedulerBackend: Granted executor ID app-20210626144933-0005/0 on hostPort 10.244.6.143:38083 with 2 core(s), 2000.0 MB RAM\n",
      "21/06/26 14:49:34 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210626144933-0005/0 is now RUNNING\n",
      "21/06/26 14:49:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.6.143:44736) with ID 0\n",
      "21/06/26 14:49:37 INFO ExecutorAllocationManager: New executor 0 has registered (new total is 1)\n",
      "21/06/26 14:49:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.244.6.143, executor 0, partition 0, PROCESS_LOCAL, 8439 bytes)\n",
      "21/06/26 14:49:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.6.143:33347 with 886.8 MB RAM, BlockManagerId(0, 10.244.6.143, 33347, None)\n",
      "21/06/26 14:49:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.244.6.143:33347 (size: 4.5 KB, free: 886.8 MB)\n",
      "21/06/26 14:49:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.244.6.143:33347 (size: 24.3 KB, free: 886.8 MB)\n",
      "21/06/26 14:49:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2570 ms on 10.244.6.143 (executor 0) (1/1)\n",
      "21/06/26 14:49:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/06/26 14:49:40 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 6.507 s\n",
      "21/06/26 14:49:40 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 6.555496 s\n",
      "21/06/26 14:49:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on livy-spark-6c796bc968-n8v52:43041 in memory (size: 4.5 KB, free: 366.3 MB)\n",
      "21/06/26 14:49:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.244.6.143:33347 in memory (size: 4.5 KB, free: 886.8 MB)\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 19\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 12\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 21\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 26\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 23\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 25\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 15\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 13\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 7\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 5\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 20\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 10\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 18\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 8\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 9\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 27\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 16\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 31\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 30\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 17\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 28\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 11\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 24\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 29\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 3\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 1\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 22\n",
      "21/06/26 14:49:40 INFO ContextCleaner: Cleaned accumulator 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-----splitting data\")\n",
    "train_data,test_data = dataset.randomSplit([0.8,0.2])\n",
    "\n",
    "print(\"-----creating pipeline\")\n",
    "stage_1 = VectorAssembler(inputCols=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'], outputCol = 'Attributes')\n",
    "# define stage 5: logistic regression model                          \n",
    "stage_2 = LinearRegression(featuresCol = 'Attributes', labelCol = 'medv')\n",
    "\n",
    "# setup the pipeline\n",
    "regression_pipeline = Pipeline(stages= [stage_1, stage_2])\n",
    "print(\"-----training pipeline\")\n",
    "regression_pipeline = regression_pipeline.fit(train_data)\n",
    "test_update = regression_pipeline.transform(test_data)\n",
    "# test_update.show()\n",
    "\n",
    "#Predict using the model\n",
    "print(\"-----test data\")\n",
    "test_update.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define score function\n",
    "def score(model, request):\n",
    "    data = request.json\n",
    "    df = pd.DataFrame(data, columns=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat', 'medv'])\n",
    "    spark = SparkSession.builder.appName(\"jupyter-nb-test\").getOrCreate()\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    return model.transform(spark_df).toPandas()[\"prediction\"].to_json()\n",
    "\n",
    "print(\"-----defining & testing score function\")\n",
    "import requests\n",
    "req = requests.Request()\n",
    "req.json = [[1, -0.1, 0.0425, 0.0279, 2.4716, -15.2808, 4.2783, -0.0005, -1.4009, 0.2692, -0.013, -0.9905, 0.0084, -0.5533]]\n",
    "rez = score(regression_pipeline, req)\n",
    "print(\"the prediction is {}\".format(rez))\n",
    "\n",
    "print(\"-----register model\")\n",
    "register_model(regression_pipeline, score, \"Boston_Regression_Analysis_jun26_2\", \"pyspark model\", MLModelFlavours.pyspark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
